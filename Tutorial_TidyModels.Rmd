---
title: "Uma Jornada pelos TidyModels em R"
author:
- name: Tatiana Benaglia
  affiliation: 
  - Departamento de Estatística - IMECC - UNICAMP
- name: Samara Kiihl
  affiliation: Departamento de Estatística - IMECC - UNICAMP
date: "August 29th 2024"  
output: 
  BiocStyle::html_document:
    toc: true
    toc_float:
        collapsed: true
        smooth_scroll: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
bibliography: refs.bib
nocite: | 
  @misc_abalone_1
  @James2013
  @hastie01statisticallearning
  @kuhn2022tidy
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Introduction

This document is part of the tutorial presented during the Artificial Intelligence for Malaria (and Infectious Diseases) Modelling in Brazil and Latin America ([AIMM 2024](https://sally.ufba.br/AIMM2024.html)).

Source files are available on <https://github.com/samarafk/AIMM2024>.


# Dataset

The dataset used in this example is available from @Adeboye2020.

We start by loading the `tidyverse` package and importing the dataset:

```{r}
library(tidyverse)
malaria <- read_csv("https://ars.els-cdn.com/content/image/1-s2.0-S2352340919313526-mmc1.csv")
```

Quick information about the dataset:
```{r}
malaria %>% glimpse()
```

We have $n=`r nrow(malaria)`$ observations. 

Let's quickly rename one of the variables:
```{r}
malaria <- malaria %>% rename(severe_malaria = severe_maleria)
```


The first 10 observations:

```{r}
malaria %>% head(10)
```

The response variable is `severe_malaria`. `tidymodels` expects the outcome to be a factor and it also treats the first level as the event:

```{r}
malaria <- malaria %>% 
  mutate(severe_malaria = factor(severe_malaria)) %>% 
  mutate(severe_malaria = relevel(severe_malaria, ref = "1"))
```


# Data Splitting

We now load the `tidymodels` package to start our analysis. The first step is to split the data into training and testing datasets. 

We use the function `initial_split()`. By default, this function splits the data so that 75\% is training and 25\% is testing.  Here, we use 80\% for training, so we specify this by setting `prop = 0.8`. We need to use `set.seed()` here, since it allows us to reproduce the results.


```{r}
library(tidymodels)
set.seed(1234)
malaria_split <- initial_split(malaria, prop = 0.8, strata = severe_malaria)
```

The functions `training()` and `testing()` are used to extract the training and testing datasets, respectively:

```{r}
malaria_train <- training(malaria_split)
malaria_test <- testing(malaria_split)
```



What is the distribution of the outcome for the training and testing datasets? 

```{r}
malaria_train %>% 
  ggplot(aes(x = severe_malaria)) +
  geom_bar()
```

Is the age distribution similar between the two malaria severity conditions?

```{r}
malaria_train %>% 
  ggplot(aes(x = age, fill = severe_malaria, group = severe_malaria)) +
  geom_density(position = "identity", alpha = .6)

```

A amostragem estratificada é feita através do argumento `strata`. A estratificação garante que os dados de teste mantenham uma distribuição próxima a dos dados de treinamento.

Como a separação em treinamento e teste é feita de forma aleatorizada é importante usar `set.seed()` para garantirmos sempre a mesma divisão dos dados ao executarmos o código novamente:




Mais detalhes sobre divisão do conjunto de dados em treinamento e teste são discutidos nos livros de @hastie01statisticallearning e @James2013.


# Model

No `tidymodels`, temos alguns passos para especificar um modelo:

1) Escolha um modelo (*model*)
2) Especifique um mecanismo (*engine*)
3) Defina o modo (*mode*)


Por exemplo, se quisermos especificar um modelo de regressão logística:

```{r}
logistic_reg()
```


Depois que a forma funcional do modelo é especificada, é necessário pensar em um mecanismo/método para ajustar ou treinar o modelo, chamado de *engine*.

```{r}
args(logistic_reg)
```


Veja que o método *default* para `logistic_reg()` é `glm` (generalized linear models), mas é possível escolher outros métodos.

Por exemplo, um modelo de regressão logística via lasso and elastic-net regularized generalized linear models:
```{r}
logistic_reg() %>% 
   set_engine("glmnet") 
```

Todos os modelos disponíveis são listados no site: <https://www.tidymodels.org/find/parsnip/>


# Recipes

Antes de proceder para o ajuste/treinamento do modelo, faz-se o pré-processamento dos dados, já que:

- Alguns **modelos** exigem que os preditores tenham certas características ou certos formatos.

- Algumas **variáveis** requerem algum tipo de transformação.

Para isso, o `tidymodels` usa o que é chamado de receita (`recipe`).

Uma primeira receita: 

```{r}
malaria_rec <- recipe(severe_malaria ~ ., data = malaria_train)
```

No exemplo acima, a função `recipe()` apenas define a variável resposta e os preditores através da fórmula.

```{r}
malaria_rec %>% summary()
```




Os passos de pré-processamento de uma receita usam os dados de treinamento para os cálculos. Tipos de cálculos no processamento:
* Níveis de uma variável categórica
* Avaliar se uma coluna tem variância nula (constante) - `step_zv()`
* Calcular média e desvio-padrão para a normalização
* Projetar os novos dados nas componentes principais obtidas pelos dados de treinamento.

Um outro exemplo de receita:

```{r}
malaria_rec <- recipe(severe_malaria ~ ., data = malaria_train) %>%
  step_poly(age, degree = 2)
```


A tabela a seguir apresenta os dados pré-processamos segundo a receita acima:
```{r, echo=FALSE}
kable_recipe <- function(rec) {
  rec %>%
    prep() %>%
    juice() %>%
    head(10) %>%
    select(severe_malaria, everything()) %>%
    kableExtra::kable(booktabs = TRUE, digits = 3, linesep = "") %>%
    kableExtra::kable_styling(font_size = 8)
}

kable_recipe(malaria_rec)
```


# Workflow

Gerenciar um modelo especificado com o `parsnip` (`model` + `engine` + `mode`) e os passos de pré-processamento usando `recipes`, pode ser um desafio.

Para isso, o `tidymodels` propõe o uso da função `workflow()`, que possui dois argumentos opcionais:

* `preprocessor`: pode ser uma fórmula ou uma receita
* `spec`: modelo especificado com `parsnip`


Vamos especificar um modelo de regressão logística:

```{r}
reg_spec <- logistic_reg() %>%
  set_engine("glm", family = "binomial") %>%
  set_mode("classification")
```

A receita a ser aplicada está no objeto `malaria_rec`.

Vamos agregar essas duas informações no *workflow*:

```{r}
reg_wf <- workflow(malaria_rec, reg_spec)
```

A função `fit()` pode ser usada para ajustar o modelo usando os dados de treinamento:

```{r}
reg_fit <- reg_wf %>% fit(data = malaria_train)
reg_fit
```


A função `tidy()` do pacote `broom` resume as informações mais importantes de um modelo usando o conceito *tidy*:

```{r}
reg_fit %>% tidy(conf.int = TRUE)
```



```{r}
reg_fit %>% tidy(conf.int = TRUE, exponentiate = TRUE)
```

A função `predict()` calcula os valores preditos para o conjunto especificado:

```{r}
reg_fit %>% predict(malaria_train)
```


A predição com `tidymodels` garante que:

- as predições estarão sempre dentro de um dataframe/tibble;
- os nomes e tipos das colunas são previsíveis e intuitivas;
- o número de linhas em new_data e a saída são iguais.

Pode-se usar também a função `augment()`, que calcula os valores preditos e resíduos, adicionando-os em uma coluna no conjunto de dados especificado:

```{r}
reg_fit %>% augment(malaria_train)
```

  
  
# Avaliar e comparar modelos

Até aqui, fizemos o pré-processamento, definimos e treinamos o modelo escolhido usando os dados de treinamento.


## Métricas

Como avaliar se o modelo tem bom desempenho (foco em predição)?


Olhar os resultados para cada observação não é produtivo:

```{r}
reg_fit %>%
  augment(malaria_train) %>% 
  head()
```

Temos algumas métricas para comparar os valores preditos com os observados (erro de predição):

-   Erro Quadrático Médio: $RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2}$
-   Coeficiente de determinação: $R^2$
-   Erro Médio Absoluto: $MAE=\frac{1}{n}\sum_{i=1}^n|y_i-\hat{y}_i|$


Dentro de `tidymodels` temos a função `metrics()` do pacote `yardstick` para avaliar o modelo. Temos que especificar os seguintes argumentos em `metrics()`:

* `truth`: nome da variável com os valores observados da resposta
* `estimate`: nome da variável contendo os valores preditos

```{r}
reg_fit %>%
  augment(new_data = malaria_train) %>%
  metrics(truth = severe_malaria, estimate = .pred_class)
```

Podemos especificar apenas uma métrica:

```{r}
reg_fit %>%
  augment(new_data = malaria_train) %>%
  accuracy(truth = severe_malaria, estimate = .pred_class)
```


We can look at the resulting confusion matrix:

```{r}
reg_fit %>%
  augment(new_data = malaria_train) %>%
  conf_mat(truth = severe_malaria, estimate = .pred_class)
```


Também é possível especificar um conjunto de métricas. No exemplo a seguir, `abalone_metrics` é definido como um conjunto de quatro métricas: RMSE, MAE, MAPE e $R^2$:

```{r}
malaria_metrics <- metric_set(accuracy, sensitivity, specificity)
```

E podemos avaliar este conjunto de métricas no modelo ajustado: 

```{r}
augment(reg_fit, new_data = malaria_train) %>%
  malaria_metrics(truth = severe_malaria, estimate = .pred_class)
```


Ao calcularmos as métricas tanto no conjunto de treinamento quanto no de teste, podemos avaliar se o modelo está super-ajustando (*overfitting*):


::: columns
::: {.column width="50%"}
```{r}
reg_fit %>%
  augment(malaria_train) %>%
  metrics(truth = severe_malaria, estimate = .pred_class)
```
:::

::: {.column width="50%"}
```{r}
reg_fit %>%
  augment(malaria_test) %>%
  metrics(truth = severe_malaria, estimate = .pred_class)
```
:::
:::


ROC CURVES

https://workshops.tidymodels.org/slides/intro-04-evaluating-models.html#/roc-curves-2

## Validação cruzada

Usaremos como exemplo 5 *folds*. Para fazer a reamostragem nos dados de treinamento, usaremos o comando `vfold_cv`:

```{r}
set.seed(234)
malaria_folds <- malaria_train %>%
                vfold_cv(v = 5, strata = severe_malaria)
malaria_folds
```

Ajustando o modelo nas reamostras:

```{r}
reg_cv <- reg_wf %>% fit_resamples(malaria_folds)
reg_cv
```

A função `collect_metrics()` extrai as métricas obtidas em cada reamostra e calcula a métrica da validação, geralmente através de uma média:

```{r}
reg_cv %>%
  collect_metrics()
```

Para calcular um conjunto escolhido de métricas, é preciso especificar o conjunto no argumento `metrics` dentro de `fit_resamples`:

```{r}
reg_cv <- fit_resamples(reg_wf, 
                 malaria_folds,
                 metrics = malaria_metrics)

reg_cv %>%
  collect_metrics()
```


Através da validação cruzada, avaliamos o desempenho do modelo apenas com os dados de treinamento, sem usar os dados de teste.

A métrica obtida no conjunto de validação pode ser tomada como uma estimativa da métrica no conjunto de teste. 

Caso seja necessário salvar as predições obtidas nas etapas de validação cruzada, para fazer um gráfico, por exemplo, usamos `control_resamples`:

```{r}
ctrl_abalone <- control_resamples(save_pred = TRUE)
reg_cv <- fit_resamples(reg_wf, 
               malaria_folds, 
               control = ctrl_abalone)
reg_preds <- collect_predictions(reg_cv)
reg_preds
```


Podemos fazer um gráfico de preditos versus observados, separados por cada *fold* da validação cruzada:

```{r,echo=TRUE, warning=FALSE}
#| fig-align: center
reg_preds %>% 
  ggplot(aes(severe_malaria, .pred, color = id)) + 
  geom_abline(lty = 2, col = "gray", size = 1.5) +
  geom_point(alpha = 0.5) +
  coord_obs_pred()
```


## Árvore de decisão

Um outro modelo a ser considerado é árvore de decisão. Vamos considerar o seguinte exemplo:

```{r}
  tree_spec <- decision_tree(cost_complexity = 0.02) %>%
      set_mode("regression") %>%
      set_engine("rpart")
```

O modelo de árvore de regressão não requer pré-processamento, de forma que podemos usar um workflow com a fórmula e o modelo especificado, por exemplo:

```{r}
tree_wf <- workflow(severe_malaria ~ ., tree_spec)
tree_wf
```

E o ajuste é com os dados de treinamento:

```{r}
tree_fit <- tree_wf %>% fit(data = malaria_train)
```  


Vamos então avaliar o desempenho da árvore de decisão usando validação cruzada:

```{r}
tree_cv <- tree_wf %>% fit_resamples(malaria_folds)
  
tree_cv %>% collect_metrics()
```  


## Conjunto de modelos

Quando queremos comparar vários modelos ao mesmo, fica muito trabalhoso fazer um por vez, como mostramos anteriormente.

Para isso, existe a função `workflow_set()` que gera um conjunto de workflows. Os argumentos desta função são:

 * `preproc`: formulas, receitas
 * `models`: modelos especificados usando `parsnip`
 
 
 
```{r}
wf_set <- workflow_set(preproc = list(rec1 = severe_malaria ~ ., rec2 = malaria_rec),
                       models = list(tree = tree_spec, reg = reg_spec),
                       cross = FALSE)
```


Agora podemos avaliar os modelos com as métricas desejadas usando a validação cruzada:


```{r}
wf_set %>%
  workflow_map("fit_resamples", resamples = malaria_folds) %>%
  rank_results()
```

E para focarmos apenas nos melhores resultados:

```{r}
wf_set %>%
  workflow_map("fit_resamples", resamples = malaria_folds) %>%
  rank_results()
```  

Se o argumento `cross = TRUE` o `workflow_set` faz um produto cruzado das receitas e modelos:

```{r}
workflow_set(preproc = list(rec1 = severe_malaria ~ ., rec2 = malaria_rec),
             models = list(tree = tree_spec, reg = reg_spec),
             cross = TRUE) %>%
  workflow_map("fit_resamples", resamples = malaria_folds) %>%
  rank_results()
```  

Suponha que o modelo de regressão linear tenha sido o escolhido.

Vamos ajustar o modelo nos dados de treinamento e verificar o desempenho nos dados de teste.

Vimos os comandos `fit()` e `predict()`/`augment()`, mas podemos usar a função `final_fit()`, que combina esses passos.

```{r}
final_fit <- last_fit(reg_wf, malaria_split) 
```

Lembre-se que o objeto `malaria_split` tem informações sobre a separação dados originais em treino e teste.

```{r}
final_fit
```

Métricas calculadas para o conjunto de dados **teste**:

```{r}
collect_metrics(final_fit) 
```

Predições para o conjunto de dados **teste**:


```{r}
collect_predictions(final_fit) %>%
  head()
```


```{r,echo=TRUE}
#| fig-align: center
collect_predictions(final_fit) %>%
  ggplot(aes(severe_malaria, .pred)) + 
  geom_abline(lty = 2, col = "deeppink4", size = 1.5) +
  geom_point(alpha = 0.5) +
  coord_obs_pred()
```

Quais informações temos em `final_fit`? 

```{r}
extract_workflow(final_fit)
```


Quais as estimativas dos parâmetros do modelo final?

```{r}
final_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

# Hiperparâmetros

Algumas características dos modelos não podem ser estimadas diretamente dos dados.

Escolhemos um modelo de regressão linear, por exemplo, e usamos os dados de treinamento para obter os parâmetros do model.

No entanto, algumas escolhas são feitas antes do ajuste do modelo: usaremos alguma forma quadrática? interações? quais variáveis vamos considerar?

Algumas decisões devem ser feitas na etapa *receita* e outras devem ser feitas *dentro do modelo*.

Para ajuste fino, podemos testar *workflows* diferentes e avaliar o desempenho com validação cruzada.

## Regressão linear com polinômio

Para um modelo de regressão linear em que uma das variáveis será considerada através de um polinômio, temos a seguinte receita:


```{r}
#| code-line-numbers: "4"
malaria_rec <-
  recipe(severe_malaria ~ ., data = malaria_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_poly(age, degree = tune())
```

Repare que, acima, não especificamos diretamente o grau do polinômio. Vamos escolher o melhor hiperparâmetro usando a função `tune()`.

Com a receita definida, vamos agregar as informações:


```{r}
regpol_wf <- workflow(malaria_rec, linear_reg())
regpol_wf
```

A função `tune_grid()` calcula um conjunto de métricas usando validação cruzada para avaliar o desempenho em um conjunto pré-determinado de hiperparâmetros de um modelo ou de uma receita:

```{r}
set.seed(123)
regpol_res <- tune_grid(regpol_wf, malaria_folds, grid = tibble(degree=1:6))
regpol_res
```

Apresentando os resultados (média dos 5 *folds*) para cada grau de polinômio (hiperparâmetro) considerado:


```{r}
collect_metrics(regpol_res)
```

Visualização gráfica usando `autoplot()`:

```{r}
#| fig-align: 'center'
autoplot(regpol_res, metric = "accuracy")
```

Mostrando os melhores resultados:
```{r}
show_best(regpol_res, metric = "accuracy", n = 3)
```


E se ajustarmos um outro modelo, também com algum outro hiperparâmetro? Como comparar?


## Árvore de decisão

A seguir temos um *workflow* para árvore de decisão com ajuste de hiperparâmetro.

Primeiro, o modelo é especificado incluindo `tune()`:
```{r}
tree_spec <-
  decision_tree(
    cost_complexity = tune()
  ) %>%
  set_mode("classification") %>% 
  set_engine("rpart")
```

A receita é definida:
```{r}
tree_rec <- 
  recipe(severe_malaria ~ ., data = malaria_train) %>%
  step_dummy(all_nominal_predictors())
```


Essas duas informações são agregadaas com *workflow*:
```{r}
tree_wf <- workflow(tree_rec, tree_spec) 
```

Vamos usar `tune_grid()` para avaliar vários valores para o hiperparâmwtro:

```{r tree-tune}
set.seed(9)
tree_res <-
  tune_grid(tree_wf, 
            resamples = malaria_folds, 
            grid = 15,
            metrics = abalone_metrics)
```

É possível fornecer um `data.frame` na opção `grid`, para ser mais específico.

Métricas obtidas através de validação cruzada considerando os valores de `grid`: 

```{r}
tree_res %>% collect_metrics()
```

Em resumo, o melhor resultado para regressão polinomial, segundo a validação cruzada:

```{r}
regpol_res %>% 
  show_best(metric = "accuracy", n = 1) %>% 
  select(.metric, .estimator, mean, n, std_err, .config)
```


E melhor resultado para regressão polinomial, segundo a validação cruzada:

```{r xgboost-best}
tree_res %>% 
  show_best(metric = "accuracy", n = 1) %>% 
  select(.metric, .estimator, mean, n, std_err, .config)
```

Entre as duas opções (regressão polinomial e árvore), vamos ficar com a regressão polinomial. Vamos selecionar o modelo com hiperparâmtro de melhor desempenho: 

```{r}
best_rmse <- select_best(regpol_res, metric = "accuracy")
best_rmse
```

E ajustando o modelo final: 


```{r}
final_res <-
  regpol_wf %>% 
  finalize_workflow(best_rmse) %>%
  last_fit(malaria_split)
final_res
```


`last_fit()` ajusta o modelo final com os dados de treino e avalia o desempenho nos dados de teste. 

Resultado no conjunto de teste:

```{r test-res}
final_res %>% collect_metrics()
```


*Workflow* final

Guardar todos os passos do ajuste final (obtidos usando o conjunto de treinamento):
```{r}
fitted_wf <- extract_workflow(final_res)
fitted_wf
```

Obter valores preditos:
```{r}
predict(fitted_wf, malaria_test[1:3,])
```

*Workflow* final:


```{r}
final_res %>% 
  extract_fit_parsnip() %>%
  tidy()
```

# Referências

