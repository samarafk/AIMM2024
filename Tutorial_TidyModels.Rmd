---
title: "TidyModels in R with Malaria Data Application"
author:
- name: Tatiana Benaglia
  affiliation: 
  - Departament of Statistics - IMECC - UNICAMP
- name: Samara Kiihl
  affiliation: epartament of Statistics - IMECC - UNICAMP
date: "August 29th 2024"  
output: 
  BiocStyle::html_document:
    toc: true
    toc_float:
        collapsed: true
        smooth_scroll: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
bibliography: refs.bib
nocite: | 
  @James2013
  @hastie01statisticallearning
  @kuhn2022tidy
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Introduction

This document is part of the tutorial presented during the Artificial Intelligence for Malaria (and Infectious Diseases) Modelling in Brazil and Latin America ([AIMM 2024](https://sally.ufba.br/AIMM2024.html)) at [IMECC, UNICAMP](https://ime.unicamp.br/).

Source files are available on <https://github.com/samarafk/AIMM2024>.


# Dataset

The dataset used in this example is available from @Adeboye2020.

We start by loading the `tidyverse` package and importing the dataset:

```{r}
library(tidyverse)
malaria <- read_csv("https://ars.els-cdn.com/content/image/1-s2.0-S2352340919313526-mmc1.csv")
```

Quick information about the dataset:
```{r}
malaria %>% glimpse()
```

We have $n=`r nrow(malaria)`$ observations. 

Let's quickly rename one of the variables:
```{r}
malaria <- malaria %>% rename(severity = severe_maleria)
```


The first 10 observations:

```{r}
malaria %>% head(10)
```

The response variable is `severity`. `tidymodels` expects the outcome to be a factor and it also treats the first level as the event:

```{r}
malaria <- malaria %>% 
  mutate(severity = factor(severity)) %>% 
  mutate(severity = relevel(severity, ref = "1"))
```


# Data Splitting

We now load the `tidymodels` package to start our analysis. The first step is to split the data into training and testing datasets. 

We use the function `initial_split()`. By default, this function splits the data so that 75\% is training and 25\% is testing.  Here, we use 80\% for training, so we specify this by setting `prop = 0.8`. We need to use `set.seed()` here, since it allows us to reproduce the results.


```{r}
library(tidymodels)
tidymodels_prefer()

set.seed(1234)
malaria_split <- initial_split(malaria, prop = 0.8, strata = severity)
```

The functions `training()` and `testing()` are used to extract the training and testing datasets, respectively:

```{r}
malaria_train <- training(malaria_split)
malaria_test <- testing(malaria_split)
```



What is the distribution of the outcome for the training and testing datasets? 

```{r}
malaria_train %>% 
  ggplot(aes(x = severity)) +
  geom_bar()
```

Is the age distribution similar between the two malaria severity conditions?

```{r}
malaria_train %>% 
  ggplot(aes(x = age, fill = severity, group = severity)) +
  geom_density(position = "identity", alpha = .6)

```

A amostragem estratificada é feita através do argumento `strata`. A estratificação garante que os dados de teste mantenham uma distribuição próxima a dos dados de treinamento.

Como a separação em treinamento e teste é feita de forma aleatorizada é importante usar `set.seed()` para garantirmos sempre a mesma divisão dos dados ao executarmos o código novamente:




Mais detalhes sobre divisão do conjunto de dados em treinamento e teste são discutidos nos livros de @hastie01statisticallearning e @James2013.


# Model

We have the following steps when using `tidymodels` to specify a model:

1) Choose a *model*
2) Specify and *engine*
3) Set the *mode*


For instance, if we want to specify a logistic regression model, we use:

```{r}
logistic_reg()
```
After specifying the funcional form of the model, we need to choose the engine:

```{r}
args(logistic_reg)
```


Here we see that the *default* for `logistic_reg()` is `glm` (generalized linear models), but other models are also available.


Is we want a logistic regression model via lasso and elastic-net regularized generalized linear models:
```{r}
logistic_reg() %>% 
   set_engine("glmnet") 
```

All available models are listed in the website: <https://www.tidymodels.org/find/parsnip/>


# Recipes

Before proceeding to adjust/train the model, the data is pre-processed:

- Some **models** require predictors to have certain characteristics or certain formats.

- Some **variables** require some type of transformation.

To do this, `tidymodels` uses what is called a recipe (`recipe`).

First simple recipe:

```{r}
malaria_rec <- recipe(severity ~ ., data = malaria_train)
```

In the example above, the `recipe()` function defines the response variable and the predictors through the formula.

```{r}
malaria_rec %>% summary()
```




The preprocessing steps of a recipe use the training data for calculations. Types of calculations in processing:


* Levels of a categorical variable
* Evaluate whether a column has zero (constant) variance - `step_zv()`
* Calculate mean and standard deviation for normalization
* Transform the new data onto the principal components obtained by the training data.

Another recipe example:

```{r}
malaria_rec <- recipe(severity ~ ., data = malaria_train) %>%
  step_poly(age, degree = 2)
```


The following table presents the data pre-processed according to the recipe above:
```{r, echo=FALSE}
kable_recipe <- function(rec) {
  rec %>%
    prep() %>%
    juice() %>%
    head(10) %>%
    select(severity, everything()) %>%
    kableExtra::kable(booktabs = TRUE, digits = 3, linesep = "") %>%
    kableExtra::kable_styling(font_size = 8)
}

kable_recipe(malaria_rec)
```


# Workflow

Managing a model specified with `parsnip` (`model` + `engine` + `mode`) and preprocessing steps using `recipes` can be challenging.

To do this, `tidymodels` proposes the use of the `workflow()` function, which has two optional arguments:

* `preprocessor`: can be a formula or a recipe
* `spec`: model specified with `parsnip`


Let's specify a logistic regression model:

```{r}
logreg_spec <- logistic_reg() %>%
  set_engine("glm", family = "binomial") %>%
  set_mode("classification")
```

The recipe to be applied is in the `malaria_rec` object.

Let's add these two pieces of information to the *workflow*:

```{r}
logreg_wf <- workflow(malaria_rec, logreg_spec)
```

The `fit()` function can be used to fit the model using the training data:

```{r}
logreg_fit <- logreg_wf %>% fit(data = malaria_train)
logreg_fit
```


The `tidy()` function from the `broom` package summarizes the most important information of a model using the *tidy* concept:

```{r}
logreg_fit %>% tidy(conf.int = TRUE)
```



```{r}
logreg_fit %>% tidy(conf.int = TRUE, exponentiate = TRUE)
```

The `predict()` function calculates the predicted values for the specified dataset:

```{r}
logreg_fit %>% predict(malaria_train)
```


Prediction with `tidymodels` ensures that:

- predictions will always be within a dataframe/tibble;
- column names and types are predictable and intuitive;
- the number of lines in `new_data` and the output are the same.

You can also use the `augment()` function, which calculates the predicted values and residuals, adding them into a column in the specified data set:

```{r}
logreg_fit %>% augment(malaria_train)
```

  
  
# Evaluating and Comparing Models

So far, we have pre-processed, defined and trained the chosen model using the training data.


## Metrics

How to evaluate whether the model has good performance (here our focus is on prediction)?

Looking at the results for each observation is not productive:

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>% 
  head()
```

We have some metrics to compare predicted values with observed values (prediction error):


-   Confusion matrix
-   Accuracy
-   Sensitivity
-   Specificity


Inside `tidymodels` we have the `metrics()` function from the `yardstick` package to evaluate the model. We have to specify the following arguments in `metrics()`:

* `truth`: name of the variable with the observed values of the response
* `estimate`: name of the variable containing the predicted values

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>%
  metrics(truth = severity, estimate = .pred_class)
```

We can specify just one metric:

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>%
  accuracy(truth = severity, estimate = .pred_class)
```


We can look at the resulting confusion matrix:

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>%
  conf_mat(truth = severity, estimate = .pred_class)
```

And also make a plot to illustrate the results:

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>%
  conf_mat(truth = severity, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```




We can also specify a set of metrics. In the following example, `malaria_metrics` is defined as a set of three metrics:

```{r}
malaria_metrics <- metric_set(accuracy, sensitivity, specificity)
```

And we can evaluate this set of metrics in the fitted model:
```{r}
augment(logreg_fit, new_data = malaria_train) %>%
  malaria_metrics(truth = severity, estimate = .pred_class)
```


By calculating metrics on both the training and testing sets, we can assess whether the model is overfitting:


::: columns
::: {.column width="50%"}
```{r}
logreg_fit %>%
  augment(malaria_train) %>%
  metrics(truth = severity, estimate = .pred_class)
```
:::

::: {.column width="50%"}
```{r}
logreg_fit %>%
  augment(malaria_test) %>%
  metrics(truth = severity, estimate = .pred_class)
```
:::
:::


This metrics use the 50\% threshold to declare an event (if the estimated probability is above 0.5, we declare severity = 1 and 0 otherwise.)

What if we decide to set the probability to declare an event equals to 80\%? Varying this threshold affects sensitivity and specificity metrics.

We can use ROC (receiver operator characteristic):

* x-axis: the false positive rate (1 - specificity)
* y-axis: the true positive rate (sensitivity)

so we have a scenario with sensitivity and specificity calculated at all possible thresholds.

The area under the ROC curve can be used as a metric. 

```{r}
augment(logreg_fit, new_data = malaria_train) %>% 
  roc_auc(truth = severity, .pred_1)
```


And select a few *slices* to see their values:
```{r}
augment(logreg_fit, new_data = malaria_train) %>% 
  roc_curve(truth = severity, .pred_1) %>%
  slice(1, 20, 50)
```


## Cross Validation

We will use 5 *folds* as an example. To resample the training data, we will use the `vfold_cv` command:


```{r}
set.seed(234)
malaria_folds <- malaria_train %>%
                vfold_cv(v = 5, strata = severity)
malaria_folds
```

Fitting the model on resamples:

```{r}
logreg_cv <- logreg_wf %>% fit_resamples(malaria_folds)
logreg_cv
```

The `collect_metrics()` function extracts the metrics obtained in each resample and calculates the validation metric, generally through an average:

```{r}
logreg_cv %>%
  collect_metrics()
```


To calculate a chosen set of metrics, you need to specify the set in the `metrics` argument within `fit_resamples`:

```{r}
logreg_cv <- fit_resamples(logreg_wf, 
                 malaria_folds,
                 metrics = malaria_metrics)

logreg_cv %>%
  collect_metrics()
```


Through cross-validation, we evaluate the model's performance only with the training data, without using the test data.


A métrica obtida no conjunto de validação pode ser tomada como uma estimativa da métrica no conjunto de teste. 

In case we need to save the predictions obtained in the cross-validation steps, to create a graph, for example, we use `control_resamples`:

```{r}
ctrl_malaria <- control_resamples(save_pred = TRUE)
logreg_cv <- fit_resamples(logreg_wf, 
               malaria_folds, 
               control = ctrl_malaria)
logreg_preds <- collect_predictions(logreg_cv)
logreg_preds
```




## Decision Tree

Another model to consider is decision tree. Let's consider the following example:

```{r}
tree_spec <- decision_tree(cost_complexity = 0.02) %>%
    set_mode("classification") %>%
    set_engine("rpart")
tree_spec
```

The decision tree model does not require preprocessing, so we can use a workflow with the specified formula and a specified model, for example:

```{r}
tree_wf <- workflow(severity ~ ., tree_spec)
tree_wf
```

And then fit with training data:

```{r}
tree_fit <- tree_wf %>% fit(data = malaria_train)
```  


Let's then evaluate the performance of the decision tree using cross-validation:

```{r}
tree_cv <- tree_wf %>% fit_resamples(malaria_folds)
  
tree_cv %>% collect_metrics()
```  


## Random Forest

We can also consider a random forest model:

```{r}
rf_spec <- rand_forest(trees = 1000) %>%
    set_mode("classification")
rf_spec
```

A simple workflow:

```{r}
rf_wf <- workflow(severity ~ ., rf_spec)
rf_wf
```

Evaluation of the metrics using cross-validation:

```{r}
set.seed(2024) #RF uses random numbers, so we need to set the seed
rf_cv <- rf_wf %>% 
  fit_resamples(malaria_folds)

rf_cv %>% collect_metrics()
```


## A Set of Models

When we want to compare several models at the same time, it becomes a lot of work to do one at a time, as we showed previously.

For this, there is the `workflow_set()` function that generates a set of workflows. The arguments of this function are:

* `preproc`: formulas, recipes
* `models`: models specified using `parsnip`
 
 
 Here, we define a set of workflows of interest:
 
```{r}
wf_set <- workflow_set(preproc = list(rec1 = severity ~ ., rec2 = malaria_rec, rec1 = severity ~ .),
                       models = list(tree = tree_spec, logreg = logreg_spec, rf = rf_spec),
                       cross = FALSE)
```


We can now evaluate the models with the desired metrics using cross-validation:


```{r}
wf_set %>%
  workflow_map("fit_resamples", resamples = malaria_folds) %>%
  rank_results()
```

And to focus only on the best results:

```{r}
wf_set %>%
  workflow_map("fit_resamples", resamples = malaria_folds) %>%
  rank_results()
```  

If the argument `cross = TRUE` the `workflow_set` makes a cross product of the recipes and models listed:

```{r}
workflow_set(preproc = list(rec1 = severity ~ ., rec2 = malaria_rec),
             models = list(tree = tree_spec, logreg = logreg_spec, rf = rf_spec),
             cross = TRUE) %>%
  workflow_map("fit_resamples", resamples = malaria_folds) %>%
  rank_results()
```  

Let's suppose the logistic regression model was chosen.

Let's fit the model on training data and check performance on testing data:

```{r}
final_fit <- last_fit(logreg_wf, malaria_split) 
```

Remember that the `malaria_split` object has information about the separation of original data into training and testing.


```{r}
final_fit
```

Metrics calculated for the **test** dataset:

```{r}
collect_metrics(final_fit) 
```

Predictions for the **test** dataset:


```{r}
collect_predictions(final_fit) %>%
  head()
```

What information do we have in `final_fit`?

```{r}
extract_workflow(final_fit)
```


What are the parameter estimates of the final model?

```{r}
final_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

# Hiperparâmetros

Algumas características dos modelos não podem ser estimadas diretamente dos dados.

Escolhemos um modelo de regressão linear, por exemplo, e usamos os dados de treinamento para obter os parâmetros do model.

No entanto, algumas escolhas são feitas antes do ajuste do modelo: usaremos alguma forma quadrática? interações? quais variáveis vamos considerar?

Algumas decisões devem ser feitas na etapa *receita* e outras devem ser feitas *dentro do modelo*.

Para ajuste fino, podemos testar *workflows* diferentes e avaliar o desempenho com validação cruzada.

## Regressão linear com polinômio

Para um modelo de regressão linear em que uma das variáveis será considerada através de um polinômio, temos a seguinte receita:


```{r}
#| code-line-numbers: "4"
malaria_rec <-
  recipe(severity ~ ., data = malaria_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_poly(age, degree = tune())
```

Repare que, acima, não especificamos diretamente o grau do polinômio. Vamos escolher o melhor hiperparâmetro usando a função `tune()`.

Com a receita definida, vamos agregar as informações:


```{r}
logregpol_wf <- workflow(malaria_rec, logistic_reg())
logregpol_wf
```

A função `tune_grid()` calcula um conjunto de métricas usando validação cruzada para avaliar o desempenho em um conjunto pré-determinado de hiperparâmetros de um modelo ou de uma receita:

```{r}
set.seed(123)
logregpol_res <- tune_grid(logregpol_wf, malaria_folds, grid = tibble(degree=1:6))
logregpol_res
```

Apresentando os resultados (média dos 5 *folds*) para cada grau de polinômio (hiperparâmetro) considerado:


```{r}
collect_metrics(logregpol_res)
```

Visualização gráfica usando `autoplot()`:

```{r}
#| fig-align: 'center'
autoplot(logregpol_res, metric = "accuracy")
```

Mostrando os melhores resultados:
```{r}
show_best(logregpol_res, metric = "accuracy", n = 3)
```


E se ajustarmos um outro modelo, também com algum outro hiperparâmetro? Como comparar?


## Árvore de decisão

A seguir temos um *workflow* para árvore de decisão com ajuste de hiperparâmetro.

Primeiro, o modelo é especificado incluindo `tune()`:
```{r}
tree_spec <-
  decision_tree(
    cost_complexity = tune()
  ) %>%
  set_mode("classification") %>% 
  set_engine("rpart")
```

A receita é definida:
```{r}
tree_rec <- 
  recipe(severity ~ ., data = malaria_train) %>%
  step_dummy(all_nominal_predictors())
```


Essas duas informações são agregadaas com *workflow*:
```{r}
tree_wf <- workflow(tree_rec, tree_spec) 
```

Vamos usar `tune_grid()` para avaliar vários valores para o hiperparâmwtro:

```{r tree-tune}
set.seed(9)
tree_res <-
  tune_grid(tree_wf, 
            resamples = malaria_folds, 
            grid = 15,
            metrics = malaria_metrics)
```

É possível fornecer um `data.frame` na opção `grid`, para ser mais específico.

Métricas obtidas através de validação cruzada considerando os valores de `grid`: 

```{r}
tree_res %>% collect_metrics()
```

Em resumo, o melhor resultado para regressão polinomial, segundo a validação cruzada:

```{r}
logregpol_res %>% 
  show_best(metric = "accuracy", n = 1) %>% 
  select(.metric, .estimator, mean, n, std_err, .config)
```


E melhor resultado para regressão polinomial, segundo a validação cruzada:

```{r xgboost-best}
tree_res %>% 
  show_best(metric = "accuracy", n = 1) %>% 
  select(.metric, .estimator, mean, n, std_err, .config)
```

Entre as duas opções (regressão polinomial e árvore), vamos ficar com a regressão polinomial. Vamos selecionar o modelo com hiperparâmtro de melhor desempenho: 

```{r}
best_rmse <- select_best(logregpol_res, metric = "accuracy")
best_rmse
```

E ajustando o modelo final: 


```{r}
final_res <-
  logregpol_wf %>% 
  finalize_workflow(best_rmse) %>%
  last_fit(malaria_split)
final_res
```


`last_fit()` ajusta o modelo final com os dados de treino e avalia o desempenho nos dados de teste. 

Resultado no conjunto de teste:

```{r test-res}
final_res %>% collect_metrics()
```


*Workflow* final

Guardar todos os passos do ajuste final (obtidos usando o conjunto de treinamento):
```{r}
fitted_wf <- extract_workflow(final_res)
fitted_wf
```

Obter valores preditos:
```{r}
predict(fitted_wf, malaria_test[1:3,])
```

*Workflow* final:


```{r}
final_res %>% 
  extract_fit_parsnip() %>%
  tidy()
```

# References

