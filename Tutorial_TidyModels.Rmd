---
title: "TidyModels in R with Malaria Data Application"
author:
- name: Tatiana Benaglia
  affiliation: 
  - Departament of Statistics - IMECC - UNICAMP
- name: Samara Kiihl
  affiliation: Departament of Statistics - IMECC - UNICAMP
date: "August 29th 2024"  
output: 
  BiocStyle::html_document:
    toc: true
    toc_float:
        collapsed: true
        smooth_scroll: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}  
bibliography: refs.bib
nocite: | 
  @James2013
  @hastie01statisticallearning
  @kuhn2022tidy
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Introduction

This document is part of the tutorial presented during the Artificial Intelligence for Malaria (and Infectious Diseases) Modelling in Brazil and Latin America ([AIMM 2024](https://sally.ufba.br/AIMM2024.html)) at [IMECC, UNICAMP](https://ime.unicamp.br/).

Source files are available on <https://github.com/samarafk/AIMM2024>.


# Dataset

The dataset used in this example is available from @Adeboye2020.

We start by loading the `tidyverse` package and importing the dataset:

```{r}
library(tidyverse)
malaria <- read_csv("https://ars.els-cdn.com/content/image/1-s2.0-S2352340919313526-mmc1.csv")
```

Quick information about the dataset:
```{r}
malaria %>% glimpse()
```

We have $n=`r nrow(malaria)`$ observations. 

Let's quickly rename one of the variables:
```{r}
malaria <- malaria %>% rename(severity = severe_maleria)
```


The first 10 observations:

```{r}
library(kableExtra)
malaria %>% head(10) %>% kable(booktabs = TRUE)
```

The response variable is `severity`. `tidymodels` expects the outcome to be a factor and it also treats the first level as the event:

```{r}
malaria <- malaria %>% 
  mutate(severity = factor(severity)) %>% 
  mutate(severity = relevel(severity, ref = "1"))
```


# Data Splitting

We now load the `tidymodels` package to start our analysis. The first step is to split the data into training and testing datasets. 

We use the function `initial_split()`. By default, this function splits the data so that 75\% is training and 25\% is testing.  Here, we use 80\% for training, so we specify this by setting `prop = 0.8`. We need to use `set.seed()` here, since it allows us to reproduce the results.


```{r}
library(tidymodels)
tidymodels_prefer()

set.seed(1234)
malaria_split <- initial_split(malaria, prop = 0.8, strata = severity)
```

The functions `training()` and `testing()` are used to extract the training and testing datasets, respectively:

```{r}
malaria_train <- training(malaria_split)
malaria_test <- testing(malaria_split)
```



What is the distribution of the outcome for the training and testing datasets? 

```{r}
malaria_train %>% 
  ggplot(aes(x = severity)) +
  geom_bar()
```

Is the age distribution similar between the two malaria severity conditions?

```{r}
malaria_train %>% 
  ggplot(aes(x = age, fill = severity, group = severity)) +
  geom_density(position = "identity", alpha = .6)

```

Stratified sampling is done through the `strata` argument. Stratification ensures that the test data maintains a distribution close to that of the training data.

As the separation between training and testing is done randomly, it is important to use `set.seed()` to always guarantee the same division of data when executing the code again:




More details about splitting the dataset into training and testing are discussed in the books by @hastie01statisticallearning and @James2013.


# Model

We have the following steps when using `tidymodels` to specify a model:

1) Choose a *model*
2) Specify and *engine*
3) Set the *mode*


For instance, if we want to specify a logistic regression model, we use:

```{r}
logistic_reg()
```
After specifying the funcional form of the model, we need to choose the engine:

```{r}
args(logistic_reg)
```


Here we see that the *default* for `logistic_reg()` is `glm` (generalized linear models), but other models are also available.


Is we want a logistic regression model via lasso and elastic-net regularized generalized linear models:
```{r}
logistic_reg() %>% 
   set_engine("glmnet") 
```

All available models are listed in the website: <https://www.tidymodels.org/find/parsnip/>


# Recipes

Before proceeding to adjust/train the model, the data is pre-processed:

- Some **models** require predictors to have certain characteristics or certain formats.

- Some **variables** require some type of transformation.

To do this, `tidymodels` uses what is called a recipe (`recipe`).

First simple recipe:

```{r}
rec_simple <- recipe(severity ~ ., data = malaria_train)
```

In the example above, the `recipe()` function defines the response variable and the predictors through the formula.

```{r}
rec_simple %>% summary() %>% kable(booktabs = TRUE)
```




The preprocessing steps of a recipe use the training data for calculations. Types of calculations in processing:


* Levels of a categorical variable
* Evaluate whether a column has zero (constant) variance - `step_zv()`
* Calculate mean and standard deviation for normalization
* Transform the new data onto the principal components obtained by the training data.

Another recipe example:

```{r}
malaria_rec <- recipe(severity ~ ., data = malaria_train) %>%
  step_poly(age, degree = 2)
```


The following table presents the data pre-processed according to the recipe above:
```{r, echo=FALSE}
kable_recipe <- function(rec) {
  rec %>%
    prep() %>%
    juice() %>%
    head(10) %>%
    select(severity, everything()) %>%
    kableExtra::kable(booktabs = TRUE, digits = 3, linesep = "") %>%
    kableExtra::kable_styling(font_size = 10)
}

kable_recipe(malaria_rec)
```


# Workflow

Managing a model specified with `parsnip` (`model` + `engine` + `mode`) and preprocessing steps using `recipes` can be challenging.

To do this, `tidymodels` proposes the use of the `workflow()` function, which has two optional arguments:

* `preprocessor`: can be a formula or a recipe
* `spec`: model specified with `parsnip`


Let's specify a logistic regression model:

```{r}
logreg_spec <- logistic_reg() %>%
  set_engine("glm", family = "binomial") %>%
  set_mode("classification")
```

The recipe to be applied is in the `malaria_rec` object.

Let's add these two pieces of information to the *workflow*:

```{r}
logreg_wf <- workflow(malaria_rec, logreg_spec)
```

The `fit()` function can be used to fit the model using the training data:

```{r}
logreg_fit <- logreg_wf %>% fit(data = malaria_train)
logreg_fit
```


The `tidy()` function from the `broom` package summarizes the most important information of a model using the *tidy* concept:

```{r}
logreg_fit %>% tidy(conf.int = TRUE) %>% 
  kable(booktabs = TRUE)
```



```{r}
logreg_fit %>% tidy(conf.int = TRUE, exponentiate = TRUE) %>% 
  kable(booktabs = TRUE)
```

The `predict()` function calculates the predicted values for the specified dataset:

```{r}
logreg_fit %>% predict(malaria_train)
```


Prediction with `tidymodels` ensures that:

- predictions will always be within a dataframe/tibble;
- column names and types are predictable and intuitive;
- the number of lines in `new_data` and the output are the same.

You can also use the `augment()` function, which calculates the predicted values and residuals, adding them into a column in the specified data set:

```{r}
logreg_fit %>% augment(malaria_train)
```

  
  
# Evaluating and Comparing Models

So far, we have pre-processed, defined and trained the chosen model using the training data.


## Metrics

How to evaluate whether the model has good performance (here our focus is on prediction)?

Looking at the results for each observation is not productive:

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>% 
  head()
```

We have some metrics to compare predicted values with observed values (prediction error):


- **Confusion Matrix**
- **Accuracy**: $\frac{TP+TN}{TP+FP+FN+TN}$
- **Sensitivity**: $\frac{TP}{TP+FN}$
- **Specificity**: $\frac{TN}{FP+TN}$
- **Brier score**: score is analogous to the mean squared error in regression models $\frac{1}{n}\sum_{i=1}^n\sum_{k=1}^C(y_{ik}-\hat{p}_{ik})^2
- **Kappa**: a similar measure to accuracy, but is normalized by the accuracy that would be expected by chance alone and is very useful when one or more classes have large frequency distributions.


Inside `tidymodels` we have the `metrics()` function from the `yardstick` package to evaluate the model. We have to specify the following arguments in `metrics()`:

* `truth`: name of the variable with the observed values of the response
* `estimate`: name of the variable containing the predicted values

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>%
  metrics(truth = severity, estimate = .pred_class) %>% 
  kable(booktabs = TRUE)
```

We can specify just one metric:

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>%
  accuracy(truth = severity, estimate = .pred_class) %>% 
  kable(booktabs = TRUE)
```


We can look at the resulting confusion matrix:

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>%
  conf_mat(truth = severity, estimate = .pred_class)
```

And also make a plot to illustrate the results:

```{r}
logreg_fit %>%
  augment(new_data = malaria_train) %>%
  conf_mat(truth = severity, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```




We can also specify a set of metrics. In the following example, `malaria_metrics` is defined as a set of three metrics:

```{r}
malaria_metrics <- metric_set(accuracy, sensitivity, specificity)
```

And we can evaluate this set of metrics in the fitted model:
```{r}
augment(logreg_fit, new_data = malaria_train) %>%
  malaria_metrics(truth = severity, estimate = .pred_class) %>% 
  kable(booktabs = TRUE)
```


By calculating metrics on both the training and testing sets, we can assess whether the model is overfitting:


::: columns
::: {.column width="50%"}
```{r}
logreg_fit %>%
  augment(malaria_train) %>%
  metrics(truth = severity, estimate = .pred_class)
```
:::

::: {.column width="50%"}
```{r}
logreg_fit %>%
  augment(malaria_test) %>%
  metrics(truth = severity, estimate = .pred_class) 
```
:::
:::


This metrics use the 50\% threshold to declare an event (if the estimated probability is above 0.5, we declare severity = 1 and 0 otherwise.)

What if we decide to set the probability to declare an event equals to 80\%? Varying this threshold affects sensitivity and specificity metrics.

We can use ROC (receiver operator characteristic):

* x-axis: the false positive rate (1 - specificity)
* y-axis: the true positive rate (sensitivity)

so we have a scenario with sensitivity and specificity calculated at all possible thresholds.

The area under the ROC curve can be used as a metric. 

```{r}
augment(logreg_fit, new_data = malaria_train) %>% 
  roc_auc(truth = severity, .pred_1) %>% 
  kable(booktabs = TRUE)
```

And select a few *slices* to see their values:
```{r}
augment(logreg_fit, new_data = malaria_train) %>% 
  roc_curve(truth = severity, .pred_1) %>%
  slice(1, 20, 50, 80, 130) %>% 
  kable(booktabs = TRUE)
```


## Cross-Validation

We will use 5 *folds* as an example. To resample the training data, we will use the `vfold_cv` command:


```{r}
set.seed(234)
malaria_folds <- malaria_train %>%
                vfold_cv(v = 5, strata = severity)
malaria_folds
```

Fitting the model on resamples:

```{r}
logreg_cv <- logreg_wf %>% fit_resamples(malaria_folds)
logreg_cv
```

The `collect_metrics()` function extracts the metrics obtained in each resample and calculates the validation metric, generally through an average:

```{r}
logreg_cv %>%
  collect_metrics() %>% 
  kable(booktabs = TRUE)
```


To calculate a chosen set of metrics, you need to specify the set in the `metrics` argument within `fit_resamples`:

```{r}
logreg_cv <- fit_resamples(logreg_wf, 
                 malaria_folds,
                 metrics = malaria_metrics)

logreg_cv %>%
  collect_metrics() %>% 
  kable(booktabs = TRUE)
```


Through cross-validation, we evaluate the model's performance only with the training data, without using the test data.


The metric obtained on the validation set can be taken as an estimate of the metric on the test set.

In case we need to save the predictions obtained in the cross-validation steps, we use `control_resamples`:

```{r}
ctrl_malaria <- control_resamples(save_pred = TRUE)

logreg_cv <- fit_resamples(logreg_wf, 
               malaria_folds, 
               control = ctrl_malaria)

logreg_preds <- collect_predictions(logreg_cv)
logreg_preds
```




## Decision Tree

Another model to consider is decision tree. Let's consider the following example:

```{r}
tree_spec <- decision_tree(cost_complexity = 0.005) %>%
    set_mode("classification") %>%
    set_engine("rpart", model = TRUE)
tree_spec
```

The decision tree model does not require preprocessing, so we can use a workflow with the specified formula and the specified model:

```{r}
tree_wf <- workflow(severity ~ ., tree_spec)
tree_wf
```

And then fit with training data:

```{r}
tree_fit <- tree_wf %>% fit(data = malaria_train)
```  

Visualization of the fitted tree model:

```{r}
library(rpart.plot)

tree_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot()
```

Let's then evaluate the performance of the decision tree using cross-validation:

```{r}
tree_cv <- tree_wf %>% fit_resamples(malaria_folds)
  
tree_cv %>% collect_metrics() %>% 
  kable(booktabs = TRUE)
```  


## Random Forest

We can also consider a random forest model. Here, we will use `set_engine` and add the argument `importance = "impurity"`, so we can have variable importance scores to help us understand which predictors are relevant.

```{r}
rf_spec <- rand_forest(trees = 1000) %>%
    set_mode("classification") %>% 
    set_engine("ranger", importance = "impurity")
rf_spec
```


A simple workflow:

```{r}
rf_wf <- workflow(severity ~ ., rf_spec)
rf_wf
```

Evaluation of the metrics using cross-validation:

```{r}
set.seed(2024) #RF uses random numbers, so we need to set the seed
rf_cv <- rf_wf %>% 
  fit_resamples(malaria_folds)

rf_cv %>% collect_metrics() %>% 
  kable(booktabs = TRUE)
```


## A Set of Models

When we want to compare several models at the same time, it becomes a lot of work to do one at a time, as we showed previously.

`tidymodels` comes handy one more time. First, we can use the `workflow_set()` function to generate a set of workflows. The arguments of this function are:

* `preproc`: formulas, recipes
* `models`: models specified using `parsnip`
 
 
 Here, we define a set of workflows of interest:
 
```{r}
wf_set <- workflow_set(preproc = list(rec1 = severity ~ ., 
                                      rec2 = malaria_rec, 
                                      rec1 = severity ~ .),
                       models = list(tree = tree_spec, 
                                     logreg = logreg_spec, 
                                     rf = rf_spec),
                       cross = FALSE)
```


To process this series of workflows, we use the `workflow_map()` function. We can evaluate the models with our chosen metrics using cross-validation by setting the option `fit_resamples`:


```{r}
wf_set %>%
  workflow_map("fit_resamples", 
               resamples = malaria_folds,
               metrics = malaria_metrics,
               seed = 2024) %>%
  rank_results()
```


If the argument `cross = TRUE` the `workflow_set` makes a cross product of the recipes and models listed:

```{r}
workflow_set(preproc = list(rec1 = severity ~ ., 
                            rec2 = malaria_rec),
             models = list(tree = tree_spec, 
                           logreg = logreg_spec, 
                           rf = rf_spec),
             cross = TRUE) %>%
  workflow_map("fit_resamples", 
               resamples = malaria_folds,
               metrics = malaria_metrics,
               seed = 2024) %>%
  rank_results()
```


Let's suppose the logistic regression model was chosen.

Let's fit the model on training data and check performance on testing data:

```{r}
final_fit <- last_fit(logreg_wf, 
                      malaria_split,
                      metrics = malaria_metrics) 
```

Remember that the `malaria_split` object has information about the separation of original data into training and testing.


```{r}
final_fit
```

Metrics calculated for the **test** dataset:

```{r}
collect_metrics(final_fit) 
```

Predictions for the **test** dataset:


```{r}
collect_predictions(final_fit) %>%
  head()
```

What information do we have in `final_fit`?

```{r}
extract_workflow(final_fit)
```


What are the parameter estimates of the final model?

```{r}
final_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

# Tuning

Some model characteristics cannot be estimated directly from the data.

We choose a logistic regression model, for example, and use the training data to obtain the model parameters.

However, some choices are made before fitting the model: will we use some quadratic form? interactions? What variables will we consider?

Some decisions must be made at the *recipe* stage and others must be made *within the model*.

For tuning, we can test different *workflows* and evaluate their performances with cross-validation.

## Polynomial Logistic Regression

For a logistic regression model in which one of the variables, `age`, will be considered through a polynomial, we have the following recipe:


```{r}
#| code-line-numbers: "4"
malaria_rec <-
  recipe(severity ~ ., data = malaria_train) %>%
  step_poly(age, degree = tune())
```

Note that, above, we did not directly specify the degree of the polynomial. We will indicate that this hyperparameter needs to be tuned by using the `tune()` function.

With the recipe defined, let's aggregate the information into a workflow:


```{r}
logregpol_wf <- workflow(malaria_rec, logreg_spec)
logregpol_wf
```

We use the `tune_grid()` function, which calculates a set of metrics using cross-validation to evaluate performance on a pre-determined set (grid) of hyperparameters from a model or recipe:


```{r}
logregpol_res <- tune_grid(logregpol_wf, 
                           malaria_folds, 
                           grid = tibble(degree=1:6))
logregpol_res
```



Presenting the results (average of the 5 *folds*) for each degree of polynomial (the tuning parameter/hyperparameter) considered:


```{r}
collect_metrics(logregpol_res)
```

Graphical visualization using `autoplot()`:

```{r}
#| fig-align: 'center'
autoplot(logregpol_res, metric = "accuracy")
```

Showing the 3 best results:
```{r}
show_best(logregpol_res, 
          metric = "accuracy", 
          n = 3)
```


What if we fit another model, also with some other hyperparameter to be tuned? How to compare?


## Decision Tree

Below is a *workflow* for a decision tree with hyperparameter (`cost_complexity`) tuning.

First, the model is specified by including `tune()`:

```{r}
tree_spec <-
  decision_tree(
    cost_complexity = tune()
  ) %>%
  set_mode("classification") %>% 
  set_engine("rpart", model = TRUE)
```


The *workflow*:
```{r}
tree_wf <- workflow(severity ~ ., tree_spec) 
```

Let's use `tune_grid()` to evaluate multiple values for the hyperparameter:

```{r}
tree_res <-
  tune_grid(tree_wf, 
            resamples = malaria_folds, 
            grid = 15,
            metrics = malaria_metrics)
```

It is possible to provide a `data.frame` in the `grid` option, to be more specific.

Metrics obtained through cross-validation considering the `grid` values:

```{r}
tree_res %>% collect_metrics()
```

```{r}
autoplot(tree_res)
```



The best result from cross-validation, considering accuracy metric:
```{r}
show_best(tree_res, metric = "accuracy", n = 1)
```


## Random Forest


A tuning parameter that is commonly chosen for random forest models is `mtry`, which is the number of predictors that will be randomly sampled at each split when creating the tree models. 

For the `ranger` engine, the [default](https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html) is `floor(sqrt(ncol(x)))`.

Let's use cross-validation to tune this parameter.

We start by specifying the model and including `tune()`:

```{r}
rf_spec <- rand_forest(trees = 1000,
                       mtry = tune()) %>%
    set_mode("classification") %>% 
    set_engine("ranger", importance = "impurity")
rf_spec
```

The workflow:

```{r}
rf_wf <- workflow(severity ~ ., rf_spec)
rf_wf
```


Let's use `tune_grid()` to evaluate the performance for multiple values of the hyperparameter using cross-validation:

```{r}
set.seed(2024)
rf_res <-
  tune_grid(rf_wf, 
            resamples = malaria_folds, 
            grid = 15,
            metrics = malaria_metrics)
```


Resulting metrics from cross-validation considering the grid values:

```{r}
rf_res %>% collect_metrics()
```


```{r}
autoplot(rf_res)
```


The best result from cross-validation, considering the accuracy metric:
```{r}
show_best(rf_res, metric = "accuracy", n = 1)
```

## Fitting the chosen tuned model


Let's assume, for instance, that between the three options (polynomial logistic regression, decision tree and random forest), we decide to use the best tuned polynomial logisitc regression. 

We then need to select the model with the best performing hyperparameter, we do so by using the `select_best()` function:

```{r}
 best_acc <- select_best(logregpol_res, metric = "accuracy")
 best_acc
```

To adjust the final model, we take the desired workflow (`logregpol_wf` in this example) and use the function `finalize_workflow()` to specify the best performing hyperparameter. The function `last_fit()` fits this final model with the training data and evaluates the performance on the test data.

```{r}
final_log_fit <- logregpol_wf %>% 
   finalize_workflow(best_acc) %>%
   last_fit(malaria_split)
final_log_fit
```



Results on test set:

```{r}
final_log_fit %>% collect_metrics()
```



For the selected final *workflow*, we can save all final adjustment steps (obtained using the training set):

```{r}
fitted_wf <- extract_workflow(final_log_fit)
fitted_wf
```

Get predicted values for the test data:
```{r}
fitted_wf %>% augment(malaria_test)
```


Results on the train set:

```{r}
fitted_wf %>% 
  augment(malaria_train) %>% 
  metrics(truth = severity, estimate = .pred_class)
```



```{r}
final_log_fit %>%
  extract_fit_engine() %>%
  tidy(conf.int = TRUE) %>% 
  kable(booktabs = TRUE)
```


## Everything Everywhere All at Once

We can define a set of workflows and then evaluate it using `tune_grid`.

```{r}
wf_tune_set <- workflow_set(preproc = list(rec1 = severity ~ ., 
                                           rec2 = malaria_rec,
                                           rec1 = severity ~ .),
                            models = list(tree = tree_spec, 
                                          logreg = logreg_spec, 
                                          rf = rf_spec),
                            cross = FALSE)
```


```{r}
set.seed(2024)
wf_tune_res <-  wf_tune_set %>% 
  workflow_map(resamples = malaria_folds, 
               grid = 40,
               metrics = malaria_metrics,
               control = control_grid(save_pred = TRUE))
```



```{r}
tune_results <- wf_tune_res %>% 
  collect_metrics()
tune_results
```

Let's select the best performing workflow, considering the accuracy:
```{r}
best_wf_id <- wf_tune_res %>% 
  rank_results() %>% 
  filter(.metric == "accuracy") %>%
  arrange(desc(mean)) %>%
  slice(1) %>%  # Select the best one
  pull(wflow_id)

best_wf_id
```


Then, we can extract the best workflow as follows:
```{r}
best_workflow <- wf_tune_set %>%
  extract_workflow(best_wf_id)
```

To finalize the selected workflow fixing the hyperparameter with best performance:
```{r}
best_workflow_final <- best_workflow %>%
  finalize_workflow(
    wf_tune_res %>%
      extract_workflow_set_result(best_wf_id) %>%
      select_best(metric = "accuracy")
  )
best_workflow_final
```


```{r}
final_wf_fit <-
  best_workflow_final %>%
  last_fit(malaria_split,
           metrics = malaria_metrics)
final_wf_fit
```


Results on test set:

```{r }
final_wf_fit %>% collect_metrics()
```




For the selected final *workflow*, we save all final adjustment steps (obtained using the training set):

```{r}
fitted_wf <- extract_workflow(final_wf_fit)
fitted_wf
```

Get predicted values for the test data:
```{r}
#predict(fitted_wf, malaria_test[1:3,])
final_wf_fit %>% augment()
```

Final *workflow*:


```{r}
fitted_wf %>% 
  extract_fit_parsnip() 
```


Visualization of the predictors importance:
```{r}
library(vip)
final_wf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 10)
```


<!-- FAZER -->

<!-- https://www.tmwr.org/workflow-sets -->

<!-- https://www.tidymodels.org/start/case-study/ -->



<!-- Results on test set: -->

<!-- ```{r test-res} -->
<!-- final_res %>% collect_metrics() -->
<!-- ``` -->


<!-- For the selected final *workflow*, we save all final adjustment steps (obtained using the training set): -->

<!-- ```{r} -->
<!-- fitted_wf <- extract_workflow(final_res) -->
<!-- fitted_wf -->
<!-- ``` -->

<!-- Get predicted values: -->
<!-- ```{r} -->
<!-- predict(fitted_wf, malaria_test[1:3,]) -->
<!-- ``` -->

<!-- Final *workflow*: -->


<!-- ```{r} -->
<!-- final_res %>%  -->
<!--   extract_fit_parsnip() %>% -->
<!--   tidy() -->
<!-- ``` -->

# References

